name: MLOps CI-CD Pipeline

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install scikit-learn==1.2.1 joblib "numpy<2.0.0" boto3 mlflow

    - name: Debug Directory Structure
      run: ls -R

    - name: Train and Package Model
      run: |
        # 1. Handle potential nested directory structure
        if [ ! -d "scripts" ] && [ -d "Production_Grade_MLOps/scripts" ]; then
          echo "Detected nested structure. Changing directory to Production_Grade_MLOps"
          cd Production_Grade_MLOps
        fi
        
        # 2. Create a robust training script on the fly
        # This ensures we have a working script regardless of repository state
        mkdir -p scripts
        echo "Creating robust training script..."
        cat <<EOF > scripts/train.py
        import argparse
        import os
        import joblib
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.linear_model import LogisticRegression

        def train(model_dir):
            print(f"Training model and saving to {model_dir}...")
            # Dummy data
            X = ["I love this product", "This is amazing", "I hate this", "Terrible service"]
            y = [1, 1, 0, 0]
            
            vectorizer = CountVectorizer()
            X_vec = vectorizer.fit_transform(X)
            
            model = LogisticRegression()
            model.fit(X_vec, y)
            
            os.makedirs(model_dir, exist_ok=True)
            model_path = os.path.join(model_dir, "model.pkl")
            vec_path = os.path.join(model_dir, "vectorizer.pkl")
            
            joblib.dump(model, model_path)
            joblib.dump(vectorizer, vec_path)
            print(f"Saved model to {model_path}")
            print(f"Saved vectorizer to {vec_path}")

        if __name__ == "__main__":
            parser = argparse.ArgumentParser()
            parser.add_argument("--model-dir", type=str, default="models")
            args = parser.parse_args()
            train(args.model_dir)
        EOF
        
        # 3. Train the model
        mkdir -p models
        python scripts/train.py --model-dir models
        
        # Verify artifacts exist before tarring
        if [ ! -f "models/model.pkl" ] || [ ! -f "models/vectorizer.pkl" ]; then
          echo "::error::Model artifacts missing from models/ directory."
          ls -R models
          exit 1
        fi
        
        # 4. Prepare model artifacts (Standard SageMaker structure with code/ dir)
        mkdir -p models/code
        
        # Force creation of a valid inference.py to ensure model_fn exists
        # We write this to models/code/inference.py (Standard location)
        printf "import joblib\nimport os\nimport json\n\n" > models/code/inference.py
        printf "def model_fn(model_dir):\n" >> models/code/inference.py
        printf "    clf = joblib.load(os.path.join(model_dir, 'model.pkl'))\n" >> models/code/inference.py
        printf "    vec = joblib.load(os.path.join(model_dir, 'vectorizer.pkl'))\n" >> models/code/inference.py
        printf "    return clf, vec\n\n" >> models/code/inference.py
        printf "def predict_fn(input_data, model):\n" >> models/code/inference.py
        printf "    clf, vec = model\n" >> models/code/inference.py
        printf "    if isinstance(input_data, str): input_data = [input_data]\n" >> models/code/inference.py
        printf "    processed_data = vec.transform(input_data)\n" >> models/code/inference.py
        printf "    return clf.predict(processed_data)\n" >> models/code/inference.py
        
        # 5. Create requirements.txt in code/ directory
        echo "joblib" > models/code/requirements.txt
        echo "numpy<2.0.0" >> models/code/requirements.txt
        
        # 6. Package everything (Standard structure: model files at root, code/ dir)
        tar -czf models/model.tar.gz -C models model.pkl vectorizer.pkl code
        
        # 7. Move models to workspace root if we are in a subdirectory
        if [ "$(pwd)" != "$GITHUB_WORKSPACE" ]; then
          mv models "$GITHUB_WORKSPACE/"
        fi

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Upload model artifacts to S3
      run: |
        aws s3 cp models/model.tar.gz \
          s3://${{ secrets.S3_BUCKET }}/amazon-reviews/model.tar.gz

    - name: Deploy to SageMaker
      env:
        MODEL_NAME: ${{ secrets.MODEL_NAME }}
        ENDPOINT_NAME: ${{ secrets.ENDPOINT_NAME }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        SAGEMAKER_ROLE_ARN: ${{ secrets.SAGEMAKER_ROLE_ARN }}
        S3_BUCKET: ${{ secrets.S3_BUCKET }}
      run: |
        if [ -z "$MODEL_NAME" ] || [ -z "$ENDPOINT_NAME" ]; then
          echo "::error::MODEL_NAME or ENDPOINT_NAME secrets are not set."
          exit 1
        fi

        # Sanitize Names (SageMaker does not allow underscores)
        # Replace '_' with '-' and remove any non-alphanumeric characters (except hyphens)
        # Also ensure it doesn't start or end with a hyphen and is within limits (30 chars for model to allow suffix)
        MODEL_NAME=$(echo "$MODEL_NAME" | tr '_' '-' | sed 's/[^a-zA-Z0-9-]//g' | sed 's/^-*//' | sed 's/-*$//' | cut -c 1-30)
        ENDPOINT_NAME=$(echo "$ENDPOINT_NAME" | tr '_' '-' | sed 's/[^a-zA-Z0-9-]//g' | sed 's/^-*//' | sed 's/-*$//' | cut -c 1-63)
        
        # Generate a unique tag for this deployment
        TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
        UNIQUE_MODEL_NAME="${MODEL_NAME}-${TIMESTAMP}"
        UNIQUE_CONFIG_NAME="${MODEL_NAME}-config-${TIMESTAMP}"
        echo "Deploying Model: $UNIQUE_MODEL_NAME"
        
        # 1. Create SageMaker Model
        aws sagemaker create-model \
          --model-name "$UNIQUE_MODEL_NAME" \
          --primary-container "Image=720646828776.dkr.ecr.${AWS_REGION}.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3,ModelDataUrl=s3://${S3_BUCKET}/amazon-reviews/model.tar.gz,Environment={SAGEMAKER_PROGRAM=inference.py}" \
          --execution-role-arn "$SAGEMAKER_ROLE_ARN" \
          --region "$AWS_REGION"
          
        # 2. Create Endpoint Configuration
        aws sagemaker create-endpoint-config \
          --endpoint-config-name "$UNIQUE_CONFIG_NAME" \
          --production-variants \
            "VariantName=AllTraffic,ModelName=$UNIQUE_MODEL_NAME,InitialInstanceCount=1,InstanceType=ml.t2.medium" \
          --region "$AWS_REGION"
          
        # 3. Create or Update Endpoint
        # Check if endpoint exists
        if aws sagemaker describe-endpoint --endpoint-name "$ENDPOINT_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
          echo "Endpoint $ENDPOINT_NAME exists. Updating..."
          aws sagemaker update-endpoint \
            --endpoint-name "$ENDPOINT_NAME" \
            --endpoint-config-name "$UNIQUE_CONFIG_NAME" \
            --region "$AWS_REGION"
        else
          echo "Endpoint $ENDPOINT_NAME does not exist. Creating..."
          aws sagemaker create-endpoint \
            --endpoint-name "$ENDPOINT_NAME" \
            --endpoint-config-name "$UNIQUE_CONFIG_NAME" \
            --region "$AWS_REGION"
        fi
        
        echo "Waiting for endpoint to be InService..."
        aws sagemaker wait endpoint-in-service \
          --endpoint-name "$ENDPOINT_NAME" \
          --region "$AWS_REGION"
