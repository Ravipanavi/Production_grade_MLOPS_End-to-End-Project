name: MLOps CI-CD Pipeline

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install scikit-learn joblib numpy boto3 mlflow

    - name: Debug Directory Structure
      run: ls -R

    - name: Train and Package Model
      run: |
        # 1. Handle potential nested directory structure
        if [ ! -d "scripts" ] && [ -d "Production_Grade_MLOps/scripts" ]; then
          echo "Detected nested structure. Changing directory to Production_Grade_MLOps"
          cd Production_Grade_MLOps
        fi
        
        # 2. Locate train.py (prefer scripts/, fall back to training/)
        TRAIN_SCRIPT=$(find . -name "train.py" -type f | head -n 1)
        if [ -z "$TRAIN_SCRIPT" ]; then
          echo "::error::train.py not found in scripts/ or training/"
          find . -name "train.py"
          exit 1
        fi
        
        echo "Using training script: $TRAIN_SCRIPT"
        mkdir -p models
        
        # 3. Train the model
        # We pass --model-dir, but if the script ignores it, files land in root.
        python "$TRAIN_SCRIPT" --model-dir models
        
        echo "Training complete. Searching for generated artifacts..."
        find . -name "*.pkl"
        
        # 4. Consolidate artifacts into models/
        # Robustly locate and move artifacts to models/ directory
        for artifact in "model.pkl" "vectorizer.pkl"; do
          find . -name "$artifact" -type f | while read f; do
            # Compare absolute paths to avoid moving the file onto itself
            if [ "$(readlink -f "$f")" != "$(readlink -f models/$artifact)" ]; then
              echo "Moving $f to models/"
              mv "$f" models/
            fi
          done
        done
        
        # Verify artifacts exist before tarring
        if [ ! -f "models/model.pkl" ] || [ ! -f "models/vectorizer.pkl" ]; then
          echo "::error::Model artifacts missing from models/ directory. Listing current directory:"
          ls -R
          exit 1
        fi
        
        # 5. Copy inference code and requirements
        # Find inference.py
        INFERENCE_SCRIPT=$(find . -name "inference.py" -type f | head -n 1)
        if [ -n "$INFERENCE_SCRIPT" ]; then 
          cp "$INFERENCE_SCRIPT" models/
        else 
          echo "::error::inference.py not found"; exit 1
        fi

        # Find requirements.txt
        REQUIREMENTS_FILE=$(find . -name "requirements.txt" -type f | head -n 1)
        if [ -n "$REQUIREMENTS_FILE" ]; then cp "$REQUIREMENTS_FILE" models/; else touch models/requirements.txt; fi
        
        # 6. Package everything
        tar -czf models/model.tar.gz -C models model.pkl vectorizer.pkl inference.py requirements.txt
        
        # 7. Move models to workspace root if we are in a subdirectory
        if [ "$(pwd)" != "$GITHUB_WORKSPACE" ]; then
          mv models "$GITHUB_WORKSPACE/"
        fi

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Upload model artifacts to S3
      run: |
        aws s3 cp models/model.tar.gz \
          s3://${{ secrets.S3_BUCKET }}/amazon-reviews/model.tar.gz

    - name: Deploy to SageMaker
      env:
        MODEL_NAME: ${{ secrets.MODEL_NAME }}
        ENDPOINT_NAME: ${{ secrets.ENDPOINT_NAME }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        SAGEMAKER_ROLE_ARN: ${{ secrets.SAGEMAKER_ROLE_ARN }}
        S3_BUCKET: ${{ secrets.S3_BUCKET }}
      run: |
        # Generate a unique tag for this deployment
        TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
        UNIQUE_MODEL_NAME="${MODEL_NAME}-${TIMESTAMP}"
        UNIQUE_CONFIG_NAME="${MODEL_NAME}-config-${TIMESTAMP}"
        
        echo "Deploying Model: $UNIQUE_MODEL_NAME"
        
        # 1. Create SageMaker Model
        aws sagemaker create-model \
          --model-name "$UNIQUE_MODEL_NAME" \
          --primary-container \
            Image=720646828776.dkr.ecr.${AWS_REGION}.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3,\
            ModelDataUrl=s3://${S3_BUCKET}/amazon-reviews/model.tar.gz \
          --execution-role-arn "$SAGEMAKER_ROLE_ARN" \
          --region "$AWS_REGION"
          
        # 2. Create Endpoint Configuration
        aws sagemaker create-endpoint-config \
          --endpoint-config-name "$UNIQUE_CONFIG_NAME" \
          --production-variants \
            VariantName=AllTraffic,\
            ModelName="$UNIQUE_MODEL_NAME",\
            InitialInstanceCount=1,\
            InstanceType=ml.m5.large \
          --region "$AWS_REGION"
          
        # 3. Create or Update Endpoint
        # Check if endpoint exists
        if aws sagemaker describe-endpoint --endpoint-name "$ENDPOINT_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
          echo "Endpoint $ENDPOINT_NAME exists. Updating..."
          aws sagemaker update-endpoint \
            --endpoint-name "$ENDPOINT_NAME" \
            --endpoint-config-name "$UNIQUE_CONFIG_NAME" \
            --region "$AWS_REGION"
        else
          echo "Endpoint $ENDPOINT_NAME does not exist. Creating..."
          aws sagemaker create-endpoint \
            --endpoint-name "$ENDPOINT_NAME" \
            --endpoint-config-name "$UNIQUE_CONFIG_NAME" \
            --region "$AWS_REGION"
        fi
